# -*- coding: utf-8 -*-
"""NLP_ASSIGNMENT2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kYD8JaiyNRXlKB-2Y7q_716ROKWQB1rg

Exploring The Data
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

"""Mount Drive"""

from google.colab import drive
drive.mount('/content/drive')

"""Reading Dataset"""

dataset =  pd.read_csv('/content/drive/My Drive/train.tsv', sep='\t')
dataset = dataset.dropna()
dataset.head()

dataset.shape

c = dict(dataset.Sentiment.value_counts())
import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib.colors import Normalize
from numpy.random import rand

objects = ('0-negative ', ' 1-somewhat negative ', ' 2-neutral ', ' 3-somewhat positive ', ' 4-positive ')
y_pos = np.arange(len(objects))
performance = [c[0],c[1],c[2],c[3],c[4]]

barlist = plt.bar(y_pos, performance, align='center')
plt.xticks(y_pos, objects, rotation='vertical')
plt.ylabel('Number of Entries')
plt.title('Rotten Tomatoes Reviews')
barlist[0].set_color('#000000')
barlist[1].set_color('#000000')
barlist[2].set_color('#000000')
barlist[3].set_color('#000000')
barlist[4].set_color('#000000')

plt.show()

"""Adjustable Parameters"""

remove_fPunct = True
fTokenizaton = True
fStopwords = True
fStemming = False
fLemmatization = True

"""Data Cleaning | Punctuations"""

import string
print(string.punctuation)

def remove_punctuation(text):
  txt_nonpunct = "".join([a for a in text if a not in string.punctuation])
  return txt_nonpunct

if remove_fPunct:
  dataset['Phrase'] = dataset['Phrase'].apply(lambda x: remove_punctuation(x))

"""Data Cleaning | Tokenization"""

import re

def tokenize(text):
  tokens = re.split('\W+', text)
  return tokens 

if fTokenizaton:
  dataset['Phrase'] = dataset['Phrase'].apply(lambda x: tokenize(x.lower()))

"""Data Cleaning | Stop Words"""

import nltk 
stopwords = nltk.corpus.stopwords.words('english')
stopwords[0:10]

def remove_stopwords(txt_tokenized):
  txt_clean = [word for word in txt_tokenized if word not in stopwords]
  return txt_clean

if fStopwords:
  dataset['Phrase'] = dataset['Phrase'].apply(lambda x: remove_stopwords(x))

"""Data Cleaning | Stemming"""

from nltk.stem import PorterStemmer
ps = PorterStemmer()

def stemming(tokenized_text):
  text = [ps.stem(word) for word in tokenized_text]
  return text

if fStemming:
  dataset['Phrase'] = dataset['Phrase'].apply(lambda x: stemming(x))

"""Data Cleaning | Lemmatization"""

wn = nltk.WordNetLemmatizer()
ps = nltk.PorterStemmer()

def lemmatization(token_txt):
  text = [wn.lemmatize(word) for word in token_txt]
  return text

if fLemmatization:
  dataset['Phrase'] = dataset['Phrase'].apply(lambda x: lemmatization(x))

dataset.head()

"""Splitting The Dataset"""

X_train, X_test, Y_train, Y_test = train_test_split(dataset['Phrase'], dataset['Sentiment'], test_size=0.3, random_state=2003)
documents = []
X_train = np.array(X_train.values.tolist())
Y_train = np.array(Y_train.values.tolist())
for i in range(len(X_train)):
  documents.append([list(X_train[i]), Y_train[i]]) 

X_test = np.array(X_test.values.tolist())
Y_test = np.array(Y_test.values.tolist())
for i in range(len(X_test)):
  documents.append([list(X_test[i]), Y_test[i]]) 

print(documents[0][0])
dataset = pd.DataFrame(documents, columns=['text', 'sentiment']) 
dataset['join'] = dataset.text.apply(' '.join)
dataset.head()

X_train, X_test, Y_train, Y_test = train_test_split(dataset['join'],  dataset['sentiment'], test_size=0.3, random_state=2003)

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(max_features = 2500)
X = vectorizer.fit_transform(dataset["join"])
Y = dataset['sentiment']


X_train = vectorizer.transform(X_train).toarray()
Y_train = Y_train 
X_test = vectorizer.transform(X_test).toarray()
Y_test = Y_test

Y_test

import keras
from keras.datasets import mnist
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Conv1D, MaxPooling1D
from keras import backend as K
import matplotlib.pyplot as plt
import numpy

X_train.shape

batch_size = 64
num_classes = 5
epochs = 20
Y_train = keras.utils.to_categorical(Y_train, num_classes)
Y_test = keras.utils.to_categorical(Y_test, num_classes)
Y_test

model = Sequential()
model.add(Conv1D(filters=64, kernel_size=1,
                  activation='relu',
                  input_shape=(2500,1)))
model.add(Conv1D(filters=64, kernel_size=1, activation='relu'))
model.add(Conv1D(filters=32, kernel_size=1, activation='relu'))
model.add(MaxPooling1D(pool_size=1))
model.add(Dropout(rate = 0.1))
model.add(Flatten())
model.add(Dense(32, activation='relu'))
model.add(Dense(num_classes, activation='softmax'))
print(model.summary())

model.compile(loss=keras.losses.categorical_crossentropy,optimizer=keras.optimizers.Adam(),metrics=['accuracy'])

from keras import backend as K

def recall_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
    recall = true_positives / (possible_positives + K.epsilon())
    return recall

def precision_m(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

def f1_m(y_true, y_pred):
    precision = precision_m(y_true, y_pred)
    recall = recall_m(y_true, y_pred)
    return 2*((precision*recall)/(precision+recall+K.epsilon()))


model.compile(loss=keras.losses.categorical_crossentropy,
              optimizer=keras.optimizers.Adadelta(),
              metrics=['accuracy',f1_m,precision_m,recall_m])

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)

history = model.fit(X_train, Y_train,
          batch_size=64,
          epochs=20)

print(history.history.keys())
plt.xticks([1,3,5,7,9,11,13,15,17,19,20])
plt.plot(history.history['loss'])
plt.plot(history.history['acc'])
plt.plot(history.history['f1_m'])
plt.plot(history.history['precision_m'])
plt.plot(history.history['recall_m'])
plt.title('Training of model')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['Loss', 'Accuracy','F1 Score','Precision','Recall'], loc='upper right')
plt.show()

model.save('1104361_1dconv_reg.h5')

import keras
model.save('1104361_1dconv_reg.h5')

# Recreate the exact same model purely from the file
model = load_model('1104361_1dconv_reg.h5', 
                   custom_objects = {'f1_m': f1_m,  'precision_m': precision_m, 'recall_m' : recall_m})

score = model.evaluate(X_test, Y_test, batch_size=batch_size, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])
print('F1 Score:', score[2])
print('Precision:', score[3])
print('Recall:', score[4])